# -*- coding: utf-8 -*-
"""run_similarity_queries.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1COWomvGjhVVzwGdXao9Y4KciDq2pmwi-
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

"""
Similarity Queries
==================

Demonstrates querying a corpus for similar documents.

"""

import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

"""Creating the Corpus
-------------------

First, we need to create a corpus to work with.
This step is the same as in the previous tutorial;
if you completed it, feel free to skip to the next section.


"""

from collections import defaultdict
from gensim import corpora

documents = [
    "Human machine interface for lab abc computer applications",
    "A survey of user opinion of computer system response time",
    "The EPS user interface management system",
    "System and human system engineering testing of EPS",
    "Relation of user perceived response time to error measurement",
    "The generation of random binary unordered trees",
    "The intersection graph of paths in trees",
    "Graph minors IV Widths of trees and well quasi ordering",
    "Graph minors A survey",
]

# remove common words and tokenize
stoplist = set('for a of the and to in'.split())
texts = [
    [word for word in document.lower().split() if word not in stoplist]
    for document in documents
]

# remove words that appear only once
frequency = defaultdict(int)
for text in texts:
    for token in text:
        frequency[token] += 1

texts = [
    [token for token in text if frequency[token] > 1]
    for text in texts
]

dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

"""Similarity interface
--------------------

In the previous tutorials on
`sphx_glr_auto_examples_core_run_corpora_and_vector_spaces.py`
and
`sphx_glr_auto_examples_core_run_topics_and_transformations.py`,
we covered what it means to create a corpus in the Vector Space Model and how
to transform it between different vector spaces. A common reason for such a
charade is that we want to determine **similarity between pairs of
documents**, or the **similarity between a specific document and a set of
other documents** (such as a user query vs. indexed documents).

To show how this can be done in gensim, let us consider the same corpus as in the
previous examples (which really originally comes from Deerwester et al.'s
`"Indexing by Latent Semantic Analysis" <http://www.cs.bham.ac.uk/~pxt/IDA/lsa_ind.pdf>`_
seminal 1990 article).
To follow Deerwester's example, we first use this tiny corpus to define a 2-dimensional
LSI space:


"""

from gensim import models
lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=2)

"""For the purposes of this tutorial, there are only two things you need to know about LSI.
First, it's just another transformation: it transforms vectors from one space to another.
Second, the benefit of LSI is that enables identifying patterns and relationships between terms (in our case, words in a document) and topics.
Our LSI space is two-dimensional (`num_topics = 2`) so there are two topics, but this is arbitrary.
If you're interested, you can read more about LSI here: `Latent Semantic Indexing <https://en.wikipedia.org/wiki/Latent_semantic_indexing>`_:

Now suppose a user typed in the query `"Human computer interaction"`. We would
like to sort our nine corpus documents in decreasing order of relevance to this query.
Unlike modern search engines, here we only concentrate on a single aspect of possible
similarities---on apparent semantic relatedness of their texts (words). No hyperlinks,
no random-walk static ranks, just a semantic extension over the boolean keyword match:


"""

doc = "Human computer interaction"
vec_bow = dictionary.doc2bow(doc.lower().split())
vec_lsi = lsi[vec_bow]  # convert the query to LSI space
print(vec_lsi)

"""In addition, we will be considering `cosine similarity <http://en.wikipedia.org/wiki/Cosine_similarity>`_
to determine the similarity of two vectors. Cosine similarity is a standard measure
in Vector Space Modeling, but wherever the vectors represent probability distributions,
`different similarity measures <http://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Symmetrised_divergence>`_
may be more appropriate.

Initializing query structures
++++++++++++++++++++++++++++++++

To prepare for similarity queries, we need to enter all documents which we want
to compare against subsequent queries. In our case, they are the same nine documents
used for training LSI, converted to 2-D LSA space. But that's only incidental, we
might also be indexing a different corpus altogether.


"""

from gensim import similarities
index = similarities.MatrixSimilarity(lsi[corpus])  # transform corpus to LSI space and index it

"""<div class="alert alert-danger"><h4>Warning</h4><p>The class :class:`similarities.MatrixSimilarity` is only appropriate when the whole
  set of vectors fits into memory. For example, a corpus of one million documents
  would require 2GB of RAM in a 256-dimensional LSI space, when used with this class.

  Without 2GB of free RAM, you would need to use the :class:`similarities.Similarity` class.
  This class operates in fixed memory, by splitting the index across multiple files on disk, called shards.
  It uses :class:`similarities.MatrixSimilarity` and :class:`similarities.SparseMatrixSimilarity` internally,
  so it is still fast, although slightly more complex.</p></div>

Index persistency is handled via the standard :func:`save` and :func:`load` functions:


"""

index.save('/tmp/deerwester.index')
index = similarities.MatrixSimilarity.load('/tmp/deerwester.index')

"""This is true for all similarity indexing classes (:class:`similarities.Similarity`,
:class:`similarities.MatrixSimilarity` and :class:`similarities.SparseMatrixSimilarity`).
Also in the following, `index` can be an object of any of these. When in doubt,
use :class:`similarities.Similarity`, as it is the most scalable version, and it also
supports adding more documents to the index later.

Performing queries
++++++++++++++++++

To obtain similarities of our query document against the nine indexed documents:


"""

sims = index[vec_lsi]  # perform a similarity query against the corpus
print(list(enumerate(sims)))  # print (document_number, document_similarity) 2-tuples

"""Cosine measure returns similarities in the range `<-1, 1>` (the greater, the more similar),
so that the first document has a score of 0.99809301 etc.

With some standard Python magic we sort these similarities into descending
order, and obtain the final answer to the query `"Human computer interaction"`:


"""

sims = sorted(enumerate(sims), key=lambda item: -item[1])
for i, s in enumerate(sims):
    print(s, documents[i])

"""The thing to note here is that documents no. 2 (``"The EPS user interface management system"``)
and 4 (``"Relation of user perceived response time to error measurement"``) would never be returned by
a standard boolean fulltext search, because they do not share any common words with ``"Human
computer interaction"``. However, after applying LSI, we can observe that both of
them received quite high similarity scores (no. 2 is actually the most similar!),
which corresponds better to our intuition of
them sharing a "computer-human" related topic with the query. In fact, this semantic
generalization is the reason why we apply transformations and do topic modelling
in the first place.

Where next?
------------

Congratulations, you have finished the tutorials -- now you know how gensim works :-)
To delve into more details, you can browse through the `apiref`,
see the `wiki` or perhaps check out `distributed` in `gensim`.

Gensim is a fairly mature package that has been used successfully by many individuals and companies, both for rapid prototyping and in production.
That doesn't mean it's perfect though:

* there are parts that could be implemented more efficiently (in C, for example), or make better use of parallelism (multiple machines cores)
* new algorithms are published all the time; help gensim keep up by `discussing them <http://groups.google.com/group/gensim>`_ and `contributing code <https://github.com/piskvorky/gensim/wiki/Developer-page>`_
* your **feedback is most welcome** and appreciated (and it's not just the code!):
  `bug reports <https://github.com/piskvorky/gensim/issues>`_ or
  `user stories and general questions <http://groups.google.com/group/gensim/topics>`_.

Gensim has no ambition to become an all-encompassing framework, across all NLP (or even Machine Learning) subfields.
Its mission is to help NLP practitioners try out popular topic modelling algorithms
on large datasets easily, and to facilitate prototyping of new algorithms for researchers.


"""

import matplotlib.pyplot as plt
import matplotlib.image as mpimg
img = mpimg.imread('run_similarity_queries.png')
imgplot = plt.imshow(img)
plt.axis('off')
plt.show()

import xml.etree.ElementTree as ET
from urllib.error import HTTPError
import urllib.request
import threading
import datetime
import random
import time
import csv
import re

date = datetime.datetime.now()
currDate = '{}/{}/{}'.format(date.day,date.month,date.year)
random.seed(datetime.datetime.now())

# Directories for the BBC news webpages I'm interested in
BBCArticleURLs = ('News',
'Modi/Indai','Modi/BJP','BJP/Congress','Modi/Rahul Gandhi','BJP/Modi',
'Congress/Rahul Gandhi')

# Directories for the CNN news webpages I'm interested in
CNNArticleURLs = ('Modi',
'Modi/Indai','Modi/BJP','BJP/Congress','Modi/Rahul Gandhi','BJP/Modi',
'Congress/Rahul Gandhi')

# Directories for the CNN news webpages I'm interested in
RTArticleURLs = ('Modi',
'Modi/Indai','Modi/BJP','BJP/Congress','Modi/Rahul Gandhi','BJP/Modi',
'Congress/Rahul Gandhi')

# Directories for the BBC news webpages I'm interested in
BBCArticleURLs = ('News',
'Modi/Indai','Modi/BJP','BJP/Congress','Modi/Rahul Gandhi','BJP/Modi',
'Congress/Rahul Gandhi')

# Directories for the CNN news webpages I'm interested in
CNNArticleURLs = ('Modi',
'Modi/Indai','Modi/BJP','BJP/Congress','Modi/Rahul Gandhi','BJP/Modi',
'Congress/Rahul Gandhi')

# Directories for the CNN news webpages I'm interested in
RTArticleURLs = ('Modi',
'Modi/Indai','Modi/BJP','BJP/Congress','Modi/Rahul Gandhi','BJP/Modi',
'Congress/Rahul Gandhi')


def getArticles(dir, website):
    try:
        if website == 'BBC':
            tree = ET.parse(source=urllib.request.urlopen('http://feeds.bbci.co.uk/news/'+dir+'/rss.xml'))
        elif website == 'CNN':
            tree = ET.parse(source=urllib.request.urlopen('http://rss.cnn.com/rss/'+dir+'.rss'))
        elif website == 'RT':
            tree = ET.parse(source=urllib.request.urlopen('https://www.rt.com/rss/'+dir))
        else:
            tree = ET.parse(source=urllib.request.urlopen('https://www.theguardian.com/sitemaps/news.xml'))
    except HTTPError as err:
        print(err)
        return None
    except ET.ParseError as err:
        return None
    else:
        # Gets the xml tree as an object which
        # is then used to extract the articles
        root = tree.getroot()
        if website != 'guardian':
            allArticles = list()
            # TODO: fix issue with filtering bad titles
            # e.g 'RT UK News' or 'CNN.com - RSS' etc
            for elem in root.iter('title'):
                allArticles.append(elem.text)
            return allArticles
        else:
            i = 0
            if dir == 'titles':
                allTitles = list()
                for elem in root.iter('{http://www.google.com/schemas/sitemap-news/0.9}title'):
                    allTitles.append(elem.text.strip())
                    i += 1
                print('{} article titles scraped'.format(i))
                return allTitles

            else:
                # Returns a list where each element is a list containing
                # the keywords for an article title
                allKeywords = list()
                tempKeywords = list()
                keywordString = ''

                for elem in root.iter('{http://www.google.com/schemas/sitemap-news/0.9}keywords'):
                    keywordString = elem.text
                    try:
                        tempKeywords = keywordString.split(',')
                    except AttributeError as err:
                        print('Guardian article {} had no keywords'.format(i))
                        tempKeywords = ['no keywords']
                    allKeywords.append(tempKeywords)
                    i += 1
                print('{} article keyword lists scraped'.format(i))
                return allKeywords


def writeCSV(articleList, dir, invalid, website):
    if invalid:
        with open('errorLog.csv', 'a', encoding="utf-8") as file:
            fields = ['date', 'website', 'dir', 'articleTitle']
            writeObj = csv.DictWriter(file, fieldnames=fields,lineterminator='\n')

            for article in articleList:
                writeObj.writerow({'date':'{}'.format(currDate),'website':'{}'.format(website), 'dir':'{}'.format(dir),'articleTitle':'{}'.format(article)})
    else:
        with open('{}infoXML.csv'.format(website), 'a', encoding="utf-8") as file:
            fields = ['date', 'dir', 'articleTitle']
            writeObj = csv.DictWriter(file, fieldnames=fields,lineterminator='\n')

            for article in articleList:
                writeObj.writerow({'date':'{}'.format(currDate),'dir':'{}'.format(dir),'articleTitle':'{}'.format(article)})


def writeGuardianCSV(allTitles, allKeywords, date, invalid):
    if invalid:
        with open('errorLog.csv', 'a', encoding='utf-8') as file:
            i = 0
            fields = ['date', 'website', 'keywordsArr', 'articleTitle']
            writeObj = csv.DictWriter(file, fieldnames=fields, delimiter=',',lineterminator='\n')

            for title, keywords in zip(allTitles, allKeywords):
                writeObj.writerow({'date':'{}'.format(currDate), 'website':'{}'.format(website), 'keywordsArr':'{}'.format(keywords),'articleTitle':'{}'.format(title)})
                i += 1
    else:
        with open('guardianInfoXML.csv', 'a', encoding='utf-8') as file:
            i = 0
            fields = ['date','keywordsArr', 'articleTitle']
            writeObj = csv.DictWriter(file, fieldnames=fields, delimiter=',',lineterminator='\n')

            for title, keywords in zip(allTitles, allKeywords):
                writeObj.writerow({'date':'{}'.format(currDate),'keywordsArr':'{}'.format(keywords),'articleTitle':'{}'.format(title)})
                i += 1


def scrape(dir, website):
    if website != 'guardian':
        allArticles = getArticles(dir, website)
        if allArticles != None:
            writeCSV(allArticles, dir, 0, website)
            if website == 'BBC' or 'guardian' or 'RT':
                print('Downloaded articles from section: {} - {}'.format(website, dir))
            elif website == 'CNN':
                print('Downloaded articles from section: {} - {}'.format(website, dir[8:]))
        else:
            badscrapeMsg = 'Error could not scrape from section: {}'.format(dir)
            badscrape = list()
            badscrape.append(badscrapeMsg)
            writeCSV(badscrape, dir, 1, website)
            print('############ Failed to download articles from section: {} ############ '.format(dir))
    # If scraping from the guardian, slightly different format
    # due to the keywords list used
    if dir == 'titles':
        titlesList = getArticles('titles', 'guardian')
        return titlesList
    if dir == 'keywords':
        keywordsList = getArticles('keywords', 'guardian')
        return keywordsList


def BBCControl():
    for target in BBCArticleURLs:
        scrape(target, 'BBC')
        time.sleep(random.random())


def CNNControl():
    for target in CNNArticleURLs:
        scrape(target, 'CNN')
        time.sleep(random.random())


def RTControl():
    for target in RTArticleURLs:
        scrape(target, 'RT')
        time.sleep(random.random())


def guardianControl():
    titlesList = scrape('titles', 'guardian')
    keywordsList = scrape('keywords', 'guardian')

    if titlesList and keywordsList != None:
        writeGuardianCSV(titlesList, keywordsList, currDate, 0)
    else:
        writeGuardianCSV(titlesList, keywordsList, currDate, 1)

def main():
    threading.Thread(target=BBCControl).start()
    threading.Thread(target=CNNControl).start()
    threading.Thread(target=guardianControl).start()
    threading.Thread(target=RTControl).start()


if __name__ == '__main__':
    main()

allArticles





from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
import random
import os
from sklearn.metrics import accuracy_score
import pickle
import sys

class FactOrFeelModel(object):
	log_model = LogisticRegression()
	vectorizer = CountVectorizer()

	def __init__(self):
		# load the model from disk
		filename = 'finalized_model.sav'
		if (sys.version_info > (3, 0)): # if python3
			with open(filename,'rb') as f:
				self.log_model = pickle.load(f, encoding='latin1')
		else:
			self.log_model = pickle.load(open(filename, 'rb'))


		#load the vectorizer from the disk
		filename2 = 'vectorizer.sav'
		if (sys.version_info > (3, 0)):	# if python 3
			with open(filename2,'rb') as f:
				self.vectorizer = pickle.load(f, encoding='latin1')
		else:
			self.vectorizer = pickle.load(open(filename2, 'rb'))

	def example(self):
		text1 = "You should be proud of yourself"
		text2 = "The lab coat is white"
		data = [text1,text2]
		print_results(data)

	# data can be of the form string or [string]
	# returns ['fact'] or ['feel']
	def make_prediction(self,data):
		prediction = ''
		if type(data) == str:
			data = [data]
			return self.log_model.predict(self.vectorizer.transform(data).toarray())
		elif type(data) == list:
			return self.log_model.predict(self.vectorizer.transform(data).toarray())
		else:
			raise ValueError("data must be either list of strings or a string but is of type " + str(type(data)))

	# text is a string
	# num_sentences_per_eval is the number of sentences for each prediction (NOT CURRENT IN USE)
	# returns the percent of feel and fact
	def evaluateText(self,text):
		factCounter = 0
		feelCounter = 0

		model = FactOrFeelModel()
		splitText = text.split('.')
		splitText.pop()
		splitText = [x+y for x,y in zip(splitText[0::2], splitText[1::2])] #each prediciton is two sentences

		preds = model.make_prediction(splitText)

		for pred in preds:
			if type(pred) != str:
				pred = pred.decode("utf-8")
			if pred == 'fact':
				factCounter+=1
			else:
				feelCounter+=1

		percentFacts = int(float(factCounter)/float(factCounter+feelCounter) * 100)
		percentFeels = int(float(feelCounter)/float(feelCounter+factCounter) * 100)

		return [percentFacts,percentFeels]

	def printEvaluations(self, percentages):
		print("facts: " + str(percentages[0]) + "% | feels: " + str(percentages[1]) + "% | Predictions accuracy: 73%")


if __name__ == "__main__":
	model = FactOrFeelModel()
	path = '/content/trump.txt'
	with open(path, 'r') as content_file:
		content = content_file.read()
		percentages = model.evaluateText(content)
		model.printEvaluations(percentages)

	# while(True):
	# 	data = raw_input("Enter a sentance. (type 'q' to quit)\n")
	# 	if data == "q":
	# 		break
	# 	print(model.make_prediction(data))

pip install spacy

article = '''
Asian shares skidded on Tuesday after a rout in tech stocks put Wall Street to the sword, while a
sharp drop in oil prices and political risks in Europe pushed the dollar to 16-month highs as investors dumped
riskier assets. MSCI’s broadest index of Asia-Pacific shares outside Japan dropped 1.7 percent to a 1-1/2
week trough, with Australian shares sinking 1.6 percent. Japan’s Nikkei dived 3.1 percent led by losses in
electric machinery makers and suppliers of Apple’s iphone parts. Sterling fell to $1.286 after three straight
sessions of losses took it to the lowest since Nov.1 as there were still considerable unresolved issues with the
European Union over Brexit, British Prime Minister Theresa May said on Monday.'''

import spacy

spacy_nlp = spacy.load('en')
document = spacy_nlp(article)

print('Original Sentence: %s' % (article))

for element in document.ents:
    print('Type: %s, Value: %s' % (element.label_, element))

class SkillsExtractorNN:

    def __init__(self, word_features_dim, dense_features_dim):

        lstm_input_phrase = keras.layers.Input(shape=(None, word_features_dim))
        lstm_input_cont = keras.layers.Input(shape=(None, word_features_dim))
        dense_input = keras.layers.Input(shape=(dense_features_dim,))

        lstm_emb_phrase = keras.layers.LSTM(256)(lstm_input_phrase)
        lstm_emb_phrase = keras.layers.Dense(128, activation='relu')(lstm_emb_phrase)

        lstm_emb_cont = keras.layers.LSTM(256)(lstm_input_cont)
        lstm_emb_cont = keras.layers.Dense(128, activation='relu')(lstm_emb_cont)

        dense_emb = keras.layers.Dense(512, activation='relu')(dense_input)
        dense_emb = keras.layers.Dense(256, activation='relu')(dense_emb)

        x = keras.layers.concatenate([lstm_emb_phrase, lstm_emb_cont, dense_emb])
        x = keras.layers.Dense(128, activation='relu')(x)
        x = keras.layers.Dense(64, activation='relu')(x)
        x = keras.layers.Dense(32, activation='relu')(x)

        main_output = keras.layers.Dense(2, activation='softplus')(x)

        self.model = keras.models.Model(inputs=[lstm_input_phrase, lstm_input_cont, dense_input],
                                        outputs=main_output)

        optimizer = keras.optimizers.Adam(lr=0.0001)

        self.model.compile(optimizer=optimizer, loss='binary_crossentropy')

SkillsExtractorNN



def fit(self, x_lstm_phrase, x_lstm_context, x_dense, y,
            val_split=0.25, patience=5, max_epochs=1000, batch_size=32):

        x_lstm_phrase_seq = keras.preprocessing.sequence.pad_sequences(x_lstm_phrase)
        x_lstm_context_seq = keras.preprocessing.sequence.pad_sequences(x_lstm_context)

        y_onehot = onehot_transform(y)

        self.model.fit([x_lstm_phrase_seq, x_lstm_context_seq, x_dense],
                       y_onehot,
                       batch_size=batch_size,
                       pochs=max_epochs,
                       validation_split=val_split,
                       callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience)])

def predict(self, x_lstm_phrase, x_lstm_context, x_dense):

  x_lstm_phrase_seq = keras.preprocessing.sequence.pad_sequences(x_lstm_phrase)
  x_lstm_context_seq = keras.preprocessing.sequence.pad_sequences(x_lstm_context)

  y = self.model.predict([x_lstm_phrase_seq, x_lstm_context_seq, x_dense])

  return y

def onehot_transform(y):

    onehot_y = []

    for numb in y:
        onehot_arr = np.zeros(2)
        onehot_arr[numb] = 1
        onehot_y.append(np.array(onehot_arr))

    return np.array(onehot_y)

from urllib.request import urlopen
url = urlopen("http://venturebeat.com/2014/07/04/facebooks-little-social-experiment-got-you-bummed-out-get-over-it/")
#contents = url.read()
html = url.read()
html[:500]


from bs4 import BeautifulSoup
soup = BeautifulSoup(html, 'html.parser')

print(soup.get_text())

from readability.readability import Document
from bs4 import BeautifulSoup

readable_article = Document(html).summary()
readable_title = Document(html).title()
soup = BeautifulSoup(readable_article)
print('*** TITLE *** \n\"' + readable_title + '\"\n')
print('*** CONTENT *** \n\"' + soup.text[:500] + '[...]\"')

import nltk
tokens = [word for sent in nltk.sent_tokenize(soup.text) for word in nltk.word_tokenize(sent)]

for token in sorted(set(tokens))[:30]:
    print(token + ' [' + str(tokens.count(token)) + ']')

from nltk.stem.snowball import SnowballStemmer

stemmer = SnowballStemmer("english")
stemmed_tokens = [stemmer.stem(t) for t in tokens]

for token in sorted(set(stemmed_tokens))[50:75]:
    print(token+ ' [' + str(stemmed_tokens.count(token)) + ']')



import requests
import nltk

page = requests.get('https://qz.com/africa/latest')
soup = BeautifulSoup(page.content, 'html.parser')

weblinks = soup.find_all('article')
pagelinks = []
for link in weblinks[5:]:
      url = link.contents[0].find_all('a')[0]
      pagelinks.append('http://qz.com'+url.get('href'))

from newspaper import Article

pip install newspaper3k

url1 = 'https://www.vox.com/platform/amp/2019/11/26/20983690/trump-impeachment-hearings-women-poll-2020-democrats'
article1 = Article(url1)

url2 = 'https://amp.cnn.com/cnn/2019/11/26/politics/trump-cnn-impeachment-poll/index.html'
article2 = Article(url2)

url3 = 'https://amp.usatoday.com/amp/4305749002'
article3 = Article(url3)

url4 = 'https://mobile.reuters.com/article/amp/idUSKBN1Y02MO'
article4 = Article(url4)

article1.download()

article2.download()

article3.download()

article4.download()

article2.html

article1.parse()



article2.parse()

article3.parse()

article4.parse()

Doc1 = article1.text
Doc2 = article2.text
Doc3 = article3.text
Doc4 = article4.text


article1.text
Doc = (Doc1,Doc2,Doc3,Doc4)

article3.nlp()
article3.keywords
article3.summary
import nltk

nltk.download('stopwords')

sentence_list1 = nltk.sent_tokenize(article1.text)
sentence_list2 = nltk.sent_tokenize(article2.text)
sentence_list3 = nltk.sent_tokenize(article3.text)
sentence_list4 = nltk.sent_tokenize(article4.text)

sentence_list=(sentence_list1,sentence_list2,sentence_list3,sentence_list4)

out = [item for t in sentence_list for item in t]
out

for art in Doc:



  stopwords = nltk.corpus.stopwords.words('english')



  word_frequencies = {}
  for word in nltk.word_tokenize(Doc3):
      if word not in stopwords:
          if word not in word_frequencies.keys():
              word_frequencies[word] = 1
          else:
              word_frequencies[word] += 1

maximum_frequncy = max(word_frequencies.values())

for word in word_frequencies.keys():
    word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)

sentence_scores = {}
#change out
for sent in sentence_list1:
    for word in nltk.word_tokenize(sent.lower()):
        if word in word_frequencies.keys():
            if len(sent.split(' ')) < 30:
                if sent not in sentence_scores.keys():
                    sentence_scores[sent] = word_frequencies[word]
                else:
                    sentence_scores[sent] += word_frequencies[word]

import heapq
summary_sentences = heapq.nlargest(40, sentence_scores, key=sentence_scores.get)

summary = ' '.join(summary_sentences)
print(summary)

article = summary

pip install -update _version
from textteaser import TextTeaser
tt = TextTeaser()
tt.summarize(title, Doc2)

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import nltk
nltk.download('punkt')
nltk.download('stopwords')
# tokenization
X_list = word_tokenize(Doc1)
Y_list = word_tokenize(Doc4)

# sw contains the list of stopwords
sw = stopwords.words('english')
l1 =[];l2 =[]

# remove stop words from string
X_set = {w for w in X_list if not w in sw}
Y_set = {w for w in Y_list if not w in sw}

# form a set containing keywords of both strings
rvector = X_set.union(Y_set)
for w in rvector:
    if w in X_set: l1.append(1) # create a vector
    else: l1.append(0)
    if w in Y_set: l2.append(1)
    else: l2.append(0)
c = 0

# cosine formula
for i in range(len(rvector)):
        c+= l1[i]*l2[i]
cosine = c / float((sum(l1)*sum(l2))**0.5)
print("similarity between article 3,4: ", cosine)

authorname = []
title = []
thearticle = []

# store the text for each article
articletext = []






aname = article.authors
# get article title

thetitle = article.title

# get text
articletext = article.text


# combine all paragraphs into an article
thearticle.append(articletext)
authorname.append(aname)
title.append(thetitle)
myarticle = [' '.join(article) for article in thearticle]

articletext

import pandas as pd
from datetime import datetime
# save article data to file
data = {'Title':article.title,
        'Author':article.authors,
        'PageLink':url,
        'Article':article.text,
        'Date':article.publish_date,
        'Summary':article.summary,
        'Keywords':listToStr}

oldnews = pd.read_excel('/content/news.xls')
news = pd.DataFrame(data=data)
cols = ['Title', 'Author', 'PageLink', 'Article', 'Date', 'Summary','Keywords']
news = news[cols]
afronews = oldnews.append(news)
afronews.drop_duplicates(subset='Title', keep='last', inplace=True)
afronews.reset_index(inplace=True)
afronews.drop(labels='index', axis=1, inplace=True)
filename = '/content/news.xls'
wks_name = 'Data'
writer = pd.ExcelWriter(filename)
afronews.to_excel(writer, wks_name, index=False)
writer.save()

#!python -m spacy link en_core_web_sm en
!python -m spacy download en_core_web_lg

!python -m spacy link en_core_web_lg

pip install textacy

!python -m textacy download depeche_mood

!python -m textacy --help

#doc = textacy.make_spacy_doc(text)
from pathlib import Path

#!python -m spacy download en_core_web_lg
#!pip install -U spacy download en_core_web_sm
import spacy
import textacy
text = Doc2




import sys

#import neuralcoref
nlp = spacy.load('en_core_web_lg')
#neuralcoref.add_to_pipe(nlp)

doc = nlp1(article3.summary)

nlp1 = spacy.load('en_core_web_lg')
doc = nlp1(text)

print(doc)

print("Named Entities")
for entity in doc.ents:
    print(f" - {entity.text} ({entity.label_})")

svos = textacy.extract.subject_verb_object_triples(doc)

# Print the results
print("Subject, verb, object tuples:")

for svo in svos:
    subject, verb, object = svo
    print(f" - {svo}")

import spacy
import textacy.extract

nlp = spacy.load('en_core_web_lg')

document = nlp(text)
document

statements = textacy.extract.semistructured_statements(doc, "President")

print("**** Information on Trump ****")
count = 1
for statement in statements:
    subject, verb, fact = statement
    print(str(count) + " - Statement: ", statement,  cue=token.lemma_ , ignore_entity_case=True)
    print(str(count) + " - Fact: ", fact)
    count += 1
for entity in doc.ents:


  print(f"{entity.text} ({entity.label_})")

text = Path("/content/trump.txt").read_text()
text

# Parse the document with spaCy
doc = nlp(text)
doc

statements = textacy.extract.semistructured_statements(doc, "Democrats")

print("Here are the things I know about Trump:\n")

for statement in statements:
    subject, verb, fact = statement
    print(f"- {fact}")



# Print the results
print("Here are the facts on Donald Trump :")
for token in doc:

  verb1 = token.lemma_

  #if token.pos_ == 'VERB':



    # Extract semi-structured statements
  statements = textacy.extract.semistructured_statements(doc, "Trump", cue=token.lemma_, ignore_entity_case=True)

  for statement in statements:


      #if token.pos_ == 'VERB':
    entity, verb, fact = statement
    print(f" - ", verb1, " " + token.text + " " + str(fact))
        #qg.generate_closed_question(doc, "impeachment proceeding", token)
        #print(f" - " + str(fact))

pip install textteaser

from gensim.test.utils import common_dictionary, common_corpus
from gensim.models import LsiModel
model = LsiModel(common_corpus, id2word=common_dictionary)
vectorized_corpus = model[common_corpus]



list(textacy.extract.semistructured_statements(doc, "Trump", cue=token.lemma_, ignore_entity_case=True))

statements = textacy.extract.semistructured_statements(Doc2,"Trump")

print("This text is about: ")
for statement in statements:
    subject,verb,point = statement
    print(f':{point}')

nlp = spacy.load('en_core_web_lg')
from pathlib import Path

text = Path("/content/trump.txt").read_text()

doc = textacy.make_spacy_doc(text,lang='en_core_web_sm')



statements = textacy.extract.semistructured_statements(doc, "Impeachment")
statements

for statement in statements:
    subject,verb,fact=statement
    print(fact)



pip install textacy

pip install -U spacy==2.1.8

from sklearn.model_selection import cross_validate
from sklearn import preprocessing, svm
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt
from matplotlib import style
import datetime
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import Dropout
import requests
from pandas_datareader import data
from sklearn.model_selection import train_test_split
#from yahoofinancials import YahooFinancials
import datetime
import sys
import warnings
import tensorflow as tf

import numpy as np
import os
import seaborn as sns
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
#from datetime import datetime
#from datetime import timedelta


from tqdm import tqdm
if not sys.warnoptions:
    warnings.simplefilter('ignore')
sns.set()
tf.compat.v1.random.set_random_seed(1234)
# To grab stock data
import yfinance as fyf
from pandas_datareader import data as pdr
fyf.pdr_override()
stocks = ["AAPL"] # If you want to grab multiple stocks add more labels to this list

# Set start and end dates
start = datetime.datetime(1995, 1, 1)
end   = datetime.datetime.now()

# Grab data
df = pdr.get_data_yahoo(stocks, start = start, end = end)
print(df.head())

minmax = MinMaxScaler().fit(df.iloc[:, 4:5].astype('float32')) # Close index
df_log = minmax.transform(df.iloc[:, 4:5].astype('float32')) # Close index
df_log = pd.DataFrame(df_log)
print(df_log.head())

simulation_size = 6
num_layers = 1
size_layer = 128
timestamp = 5
epoch = 300
dropout_rate = 0.8
test_size = 30
learning_rate = 0.01

df_train = df_log
print(df.shape, df_train.shape)


class Model:
    def __init__(
            self,
            learning_rate,
            num_layers,
            size,
            size_layer,
            output_size,
            forget_bias=0.1,
    ):
        def lstm_cell(size_layer):
            return tf.keras.layers.LSTMCell(size_layer, state_is_tuple=False)

        rnn_cells = tf.keras.layers.StackedRNNCells(
            [lstm_cell(size_layer) for _ in range(num_layers)],
            state_is_tuple=False,
        )
        self.X = tf.placeholder(tf.float32, (None, None, size))
        self.Y = tf.placeholder(tf.float32, (None, output_size))
        drop = tf.contrib.rnn.DropoutWrapper(
            rnn_cells, output_keep_prob=forget_bias
        )
        self.hidden_layer = tf.placeholder(
            tf.float32, (None, num_layers * 2 * size_layer)
        )
        self.outputs, self.last_state = tf.nn.dynamic_rnn(
            drop, self.X, initial_state=self.hidden_layer, dtype=tf.float32
        )
        self.logits = tf.layers.dense(self.outputs[-1], output_size)
        self.cost = tf.reduce_mean(tf.square(self.Y - self.logits))
        self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(
            self.cost
        )


def calculate_accuracy(real, predict):
    real = np.array(real) + 1
    predict = np.array(predict) + 1
    percentage = 1 - np.sqrt(np.mean(np.square((real - predict) / real)))
    return percentage * 100


def anchor(signal, weight):
    buffer = []
    last = signal[0]
    for i in signal:
        smoothed_val = last * weight + (1 - weight) * i
        buffer.append(smoothed_val)
        last = smoothed_val
    return buffer


def forecast():
    tf.compat.v1.reset_default_graph()
    modelnn = Model(learning_rate, num_layers, df_log.shape[1], size_layer, df_log.shape[1], dropout_rate)
    sess = tf.InteractiveSession()
    sess.run(tf.global_variables_initializer())
    date_ori = pd.to_datetime(df.iloc[:, 0]).tolist()

    pbar = tqdm(range(epoch), desc='train loop')
    for i in pbar:
        init_value = np.zeros((1, num_layers * 2 * size_layer))
        total_loss, total_acc = [], []
        for k in range(0, df_train.shape[0] - 1, timestamp):
            index = min(k + timestamp, df_train.shape[0] - 1)
            batch_x = np.expand_dims(
                df_train.iloc[k: index, :].values, axis=0
            )
            batch_y = df_train.iloc[k + 1: index + 1, :].values
            logits, last_state, _, loss = sess.run(
                [modelnn.logits, modelnn.last_state, modelnn.optimizer, modelnn.cost],
                feed_dict={
                    modelnn.X: batch_x,
                    modelnn.Y: batch_y,
                    modelnn.hidden_layer: init_value,
                },
            )
            init_value = last_state
            total_loss.append(loss)
            total_acc.append(calculate_accuracy(batch_y[:, 0], logits[:, 0]))
        pbar.set_postfix(cost=np.mean(total_loss), acc=np.mean(total_acc))

    future_day = test_size

    output_predict = np.zeros((df_train.shape[0] + future_day, df_train.shape[1]))
    output_predict[0] = df_train.iloc[0]
    upper_b = (df_train.shape[0] // timestamp) * timestamp
    init_value = np.zeros((1, num_layers * 2 * size_layer))

    for k in range(0, (df_train.shape[0] // timestamp) * timestamp, timestamp):
        out_logits, last_state = sess.run(
            [modelnn.logits, modelnn.last_state],
            feed_dict={
                modelnn.X: np.expand_dims(
                    df_train.iloc[k: k + timestamp], axis=0
                ),
                modelnn.hidden_layer: init_value,
            },
        )
        init_value = last_state
        output_predict[k + 1: k + timestamp + 1] = out_logits

    if upper_b != df_train.shape[0]:
        out_logits, last_state = sess.run(
            [modelnn.logits, modelnn.last_state],
            feed_dict={
                modelnn.X: np.expand_dims(df_train.iloc[upper_b:], axis=0),
                modelnn.hidden_layer: init_value,
            },
        )
        output_predict[upper_b + 1: df_train.shape[0] + 1] = out_logits
        future_day -= 1
        date_ori.append(date_ori[-1] + timedelta(days=1))

    init_value = last_state

    for i in range(future_day):
        o = output_predict[-future_day - timestamp + i:-future_day + i]
        out_logits, last_state = sess.run(
            [modelnn.logits, modelnn.last_state],
            feed_dict={
                modelnn.X: np.expand_dims(o, axis=0),
                modelnn.hidden_layer: init_value,
            },
        )
        init_value = last_state
        output_predict[-future_day + i] = out_logits[-1]
        date_ori.append(date_ori[-1] + timedelta(days=1))

    output_predict = minmax.inverse_transform(output_predict)
    deep_future = anchor(output_predict[:, 0], 0.4)

    return deep_future


results = []
for i in range(simulation_size):
    print('simulation %d'%(i + 1))
    results.append(forecast())
print(results)
date_ori = pd.to_datetime(df.iloc[:, 0]).tolist()
for i in range(test_size):
    date_ori.append(date_ori[-1] + timedelta(days = 1))
date_ori = pd.Series(date_ori).dt.strftime(date_format = '%Y-%m-%d').tolist()
print(date_ori[-5:])

accepted_results = []
for r in results:
    if (np.array(r[-test_size:]) < np.min(df['Close'])).sum() == 0 and \
    (np.array(r[-test_size:]) > np.max(df['Close']) * 2).sum() == 0:
        accepted_results.append(r)
print(len(accepted_results))

accuracies = [calculate_accuracy(df['Close'].values, r[:-test_size]) for r in accepted_results]

plt.figure(figsize = (15, 5))
for no, r in enumerate(accepted_results):
    plt.plot(r, label = 'forecast %d'%(no + 1))
plt.plot(df['Close'], label = 'true trend', c = 'black')
plt.legend()
plt.title('average accuracy: %.4f'%(np.mean(accuracies)))

x_range_future = np.arange(len(results[0]))
plt.xticks(x_range_future[::30], date_ori[::30])

plt.show()



pip install yfinance

import sys
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
import datetime
from datetime import timedelta
from tqdm import tqdm
sns.set()
tf.compat.v1.random.set_random_seed(1234)
import datetime
import sys
import warnings
import tensorflow as tf

import numpy as np
import os
import seaborn as sns
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
#from datetime import datetime
#from datetime import timedelta

from tqdm import tqdm
if not sys.warnoptions:
    warnings.simplefilter('ignore')
sns.set()
tf.compat.v1.random.set_random_seed(1234)
# To grab stock data
import yfinance as fyf
from pandas_datareader import data as pdr
fyf.pdr_override()
stocks = ["AAPL"] # If you want to grab multiple stocks add more labels to this list
#stocks = ["AHEALTH"]
# Set start and end dates
start = datetime.datetime(2000, 1, 1)
end   = datetime.datetime.now()

# Grab data
#df = pdr.get_data_yahoo(stocks, start = start, end = end)
#df = pd.read_csv("/content/WTK.csv")
df.index()
print(df.tail())

df['Date'] = df['Date'].astype('datetime64[ns]')
df.info()
df = df.sort_values(by=['Date'])

df.tail()

#df = df.set_index('Date')
df.to_csv("/content/WTK_.csv")

minmax = MinMaxScaler().fit(df.iloc[:, 4:5].astype('float32')) # Close index
df_log = minmax.transform(df.iloc[:, 4:5].astype('float32')) # Close index
df_log = pd.DataFrame(df_log)
df_log.head()

simulation_size = 10
num_layers = 1
size_layer = 128
timestamp = 5
epoch = 500
dropout_rate = 0.8
test_size = 7
learning_rate = 0.01

df_train = df_log
df.shape, df_train.shape

class Model:
    def __init__(
        self,
        learning_rate,
        num_layers,
        size,
        size_layer,
        output_size,
        forget_bias = 0.1,
    ):
        def lstm_cell(size_layer):
            return tf.nn.rnn_cell.LSTMCell(size_layer, state_is_tuple = False)

        rnn_cells = tf.nn.rnn_cell.MultiRNNCell(
            [lstm_cell(size_layer) for _ in range(num_layers)],
            state_is_tuple = False,
        )
        self.X = tf.placeholder(tf.float32, (None, None, size))
        self.Y = tf.placeholder(tf.float32, (None, output_size))
        drop = tf.contrib.rnn.DropoutWrapper(
            rnn_cells, output_keep_prob = forget_bias
        )
        self.hidden_layer = tf.placeholder(
            tf.float32, (None, num_layers * 2 * size_layer)
        )
        self.outputs, self.last_state = tf.nn.dynamic_rnn(
            drop, self.X, initial_state = self.hidden_layer, dtype = tf.float32
        )
        self.logits = tf.layers.dense(self.outputs[-1], output_size)
        self.cost = tf.reduce_mean(tf.square(self.Y - self.logits))
        self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(
            self.cost
        )

def calculate_accuracy(real, predict):
    real = np.array(real) + 1
    predict = np.array(predict) + 1
    percentage = 1 - np.sqrt(np.mean(np.square((real - predict) / real)))
    return percentage * 100

def anchor(signal, weight):
    buffer = []
    last = signal[0]
    for i in signal:
        smoothed_val = last * weight + (1 - weight) * i
        buffer.append(smoothed_val)
        last = smoothed_val
    return buffer

def forecast():
    tf.reset_default_graph()
    modelnn = Model(
        learning_rate, num_layers, df_log.shape[1], size_layer, df_log.shape[1], dropout_rate
    )
    sess = tf.InteractiveSession()
    sess.run(tf.global_variables_initializer())
    date_ori = pd.to_datetime(df.iloc[:, 0]).tolist()

    pbar = tqdm(range(epoch), desc = 'train loop')
    for i in pbar:
        init_value = np.zeros((1, num_layers * 2 * size_layer))
        total_loss, total_acc = [], []
        for k in range(0, df_train.shape[0] - 1, timestamp):
            index = min(k + timestamp, df_train.shape[0] - 1)
            batch_x = np.expand_dims(
                df_train.iloc[k : index, :].values, axis = 0
            )
            batch_y = df_train.iloc[k + 1 : index + 1, :].values
            logits, last_state, _, loss = sess.run(
                [modelnn.logits, modelnn.last_state, modelnn.optimizer, modelnn.cost],
                feed_dict = {
                    modelnn.X: batch_x,
                    modelnn.Y: batch_y,
                    modelnn.hidden_layer: init_value,
                },
            )
            init_value = last_state
            total_loss.append(loss)
            total_acc.append(calculate_accuracy(batch_y[:, 0], logits[:, 0]))
        pbar.set_postfix(cost = np.mean(total_loss), acc = np.mean(total_acc))

    future_day = test_size

    output_predict = np.zeros((df_train.shape[0] + future_day, df_train.shape[1]))
    output_predict[0] = df_train.iloc[0]
    upper_b = (df_train.shape[0] // timestamp) * timestamp
    init_value = np.zeros((1, num_layers * 2 * size_layer))

    for k in range(0, (df_train.shape[0] // timestamp) * timestamp, timestamp):
        out_logits, last_state = sess.run(
            [modelnn.logits, modelnn.last_state],
            feed_dict = {
                modelnn.X: np.expand_dims(
                    df_train.iloc[k : k + timestamp], axis = 0
                ),
                modelnn.hidden_layer: init_value,
            },
        )
        init_value = last_state
        output_predict[k + 1 : k + timestamp + 1] = out_logits

    if upper_b != df_train.shape[0]:
        out_logits, last_state = sess.run(
            [modelnn.logits, modelnn.last_state],
            feed_dict = {
                modelnn.X: np.expand_dims(df_train.iloc[upper_b:], axis = 0),
                modelnn.hidden_layer: init_value,
            },
        )
        output_predict[upper_b + 1 : df_train.shape[0] + 1] = out_logits
        future_day -= 1
        date_ori.append(date_ori[-1] + timedelta(days = 1))

    init_value = last_state

    for i in range(future_day):
        o = output_predict[-future_day - timestamp + i:-future_day + i]
        out_logits, last_state = sess.run(
            [modelnn.logits, modelnn.last_state],
            feed_dict = {
                modelnn.X: np.expand_dims(o, axis = 0),
                modelnn.hidden_layer: init_value,
            },
        )
        init_value = last_state
        output_predict[-future_day + i] = out_logits[-1]
        date_ori.append(date_ori[-1] + timedelta(days = 1))

    output_predict = minmax.inverse_transform(output_predict)
    deep_future = anchor(output_predict[:, 0], 0.4)

    return deep_future

results = []
for i in range(simulation_size):
    print('simulation %d'%(i + 1))
    results.append(forecast())

date_ori = pd.to_datetime(df.iloc[:, 0]).tolist()
for i in range(test_size):
    date_ori.append(date_ori[-1] + timedelta(days = 1))
date_ori = pd.Series(date_ori).dt.strftime(date_format = '%Y-%m-%d').tolist()
#date_ori[-5:]
date_ori

accepted_results = []
for r in results:
    if (np.array(r[-test_size:]) < np.min(df['Close'])).sum() == 0 and \
    (np.array(r[-test_size:]) > np.max(df['Close']) * 2).sum() == 0:
        accepted_results.append(r)
len(accepted_results)
#accepted_results

#for i in accepted_results
#df1 = pd.DataFrame(i)
#accepted_results[0]
#df1 = pd.DataFrame(accepted_results[0])
df2 = pd.DataFrame(accepted_results[1])
df3 = pd.DataFrame(accepted_results[2])
df4 = pd.DataFrame(accepted_results[3])
df5 = pd.DataFrame(accepted_results[4])
df6 = pd.DataFrame(accepted_results[5])
df7 = pd.DataFrame(accepted_results[6])
df8 = pd.DataFrame(accepted_results[7])
df9 = pd.DataFrame(accepted_results[8])

dfAll = pd.concat([df2,df3,df4,df5,df6,df7,df8,df9],axis = 1)
dfMean = dfAll.mean(axis = 1, skipna = True)
df_col = dfMean.tail(7)
df_col.to_csv('/content/sample_data/7days__WTK.csv')
#dfAll = df1[0].map(str)+df2[0].map(str)+df3[0].map(str)+df4[0].map(str)

#df_col = pd.concat([df.iloc[:, 0:0],df1], axis=1)
#df3 = df.iloc[:, 0:0].insert(df1)
#df3= pd.concat([df.iloc[:, 0:0], df1], axis=1)

#df_col.to_csv('/content/sample_data/test1.csv')

dfAll.tail(10)

j
#dfAll.tail(30)

from google.colab import drive
drive.mount('/content/drive')

accuracies = [calculate_accuracy(df['Close'].values, r[:-test_size]) for r in accepted_results]

plt.figure(figsize = (15, 5))
for no, r in enumerate(accepted_results):
    plt.plot(r, label = 'forecast %d'%(no + 1))
plt.plot(df['Close'], label = 'true trend', c = 'black')
plt.legend()
plt.title('average accuracy: %.4f'%(np.mean(accuracies)))

x_range_future = np.arange(len(results[0]))
plt.xticks(x_range_future[::30], date_ori[::30])

plt.show()

import yfinance as yf

msft = yf.Ticker("MSFT")
hist = msft.history(period="max")
hist